# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/151WRTkBDWIGIspjhiJdq7rLJIWl-497P

## IMPORT LIBRARIES
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset
from torch.utils.data import DataLoader

from tqdm import tqdm
import heapq
import csv

import numpy as np
import random
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.font_manager import FontProperties
import pandas as pd
import wandb

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

"""# DATA ANALYSIS AND PREPROCESSING"""

def data_load(path):
    # input - English
    # output - മലയാളം (Malayalam)
    df = pd.read_csv(path,header=None)
    input_data = df[0].tolist()
    output_data = df[1].tolist()
    return input_data, output_data
def create_char_set(train, val):
    char_set = set()
    for word in train:
        for char in word:
            char_set.add(char)
    for word in val:
        for char in word:
            char_set.add(char)
    return char_set

train_input, train_output = data_load("/kaggle/input/aksharantar-sampled/aksharantar_sampled/mal/mal_train.csv")
val_input, val_output = data_load("/kaggle/input/aksharantar-sampled/aksharantar_sampled/mal/mal_valid.csv")
test_input, test_output = data_load("/kaggle/input/aksharantar-sampled/aksharantar_sampled/mal/mal_test.csv")
print("Number of training samples: ", len(train_input))
print("Number of validation samples: ", len(val_input))
print("Number of test samples: ", len(test_input))

eng_chars = create_char_set(train_input, val_input)
print("Total English characters: ",len(eng_chars))
print(sorted(eng_chars))
mal_chars = create_char_set(train_output, val_output)
print("Total Malayalam characters: ",len(mal_chars))
print(sorted(mal_chars))

max_seq_eng = len(max(train_input+val_input+test_input, key=len))
max_seq_mal = len(max(train_output+val_output+test_output, key=len))
print("Length of the longest English word in corpus:",max_seq_eng)
print("Length of the longest Malayalam word in corpus::",max_seq_mal)

eng_chars_idx = {char: idx + 3 for idx, char in enumerate(sorted(eng_chars))}
eng_chars_idx['0'] = 0 # padding
eng_chars_idx['\t'] = 1 # <SOW>
eng_chars_idx['\n'] = 2 # <EOW>
print(eng_chars_idx)
mal_chars_idx = {char: idx+3 for idx, char in enumerate(sorted(mal_chars))}
mal_chars_idx['0'] = 0 # padding
mal_chars_idx['\t'] = 1 # <SOW>
mal_chars_idx['\n'] = 2 # <EOW>
print(mal_chars_idx)

idx2char_mal = {idx: char for char, idx in mal_chars_idx.items()}
mal_embedd_size = 29
eng_embedd_size = 32

def data_preprocess(data, max_seq, chars_idx):
    # Add start & end tokens and padding
    # sow = "\t" & eow = "\n"
    sow = "\t"
    eow = "\n"
    padded_data = [sow + word + "0" * (max_seq - len(word)) + eow for word in data]
    # Convert sequences to indices
    seq2idx = torch.LongTensor([[chars_idx[char] for char in seq] for seq in padded_data])
    return seq2idx

train_idx_eng = data_preprocess(train_input, max_seq_eng, eng_chars_idx)
train_idx_mal = data_preprocess(train_output, max_seq_mal, mal_chars_idx)
val_idx_eng = data_preprocess(val_input, max_seq_eng, eng_chars_idx)
val_idx_mal = data_preprocess(val_output, max_seq_mal, mal_chars_idx)
test_idx_eng = data_preprocess(test_input, max_seq_eng, eng_chars_idx)
test_idx_mal = data_preprocess(test_output, max_seq_mal, mal_chars_idx)

from torch.utils.data import DataLoader
class Dataset():
    def __init__(self, train_idx_src, train_idx_tgt):
        self.train_idx_src = train_idx_src
        self.train_idx_tgt = train_idx_tgt

    def __len__(self):
        return len(self.train_idx_src)

    def __getitem__(self, idx):
        src_sample = self.train_idx_src[idx]
        tgt_label = self.train_idx_tgt[idx]
        return src_sample, tgt_label

# Assuming train_idx_src and train_idx_tgt are lists or arrays
train_dataset = Dataset(train_idx_eng, train_idx_mal)
val_dataset = Dataset(val_idx_eng, val_idx_mal)
test_dataset = Dataset(test_idx_eng, test_idx_mal)

train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=256, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=256, shuffle=True)

"""## ENCODER PART"""

# Encoder part
class Encoder(nn.Module):
    def __init__(self,
                 input_dim = 32,
                 emb_dim = 256,
                 enc_hid_dim = 256,
                 cell_type='gru',
                 num_layers=2,
                 dropout = 0,
                 bidirectional = True):

        super(Encoder, self).__init__()
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.enc_hid_dim = enc_hid_dim
        self.num_layers = num_layers
        # Embedding the input
        self.embedding = nn.Embedding(input_dim, emb_dim)
        self.cell_type = cell_type
        # Add dropout
        self.dropout = nn.Dropout(dropout)

        # Bidirectional part
        if bidirectional:
            self.val_direction = 2
        else :
            self.val_direction = 1


        if cell_type.lower() == 'rnn':
            self.rnn = nn.RNN(input_size = emb_dim,
                              hidden_size = enc_hid_dim,
                              num_layers=num_layers,
                              dropout = dropout,
                              bidirectional= bidirectional,
                              batch_first=True)
        elif cell_type.lower() == 'lstm':
            self.rnn = nn.LSTM(input_size = emb_dim,
                              hidden_size = enc_hid_dim,
                              num_layers=num_layers,
                              dropout = dropout,
                              bidirectional= bidirectional,
                              batch_first=True)
        elif cell_type.lower() == 'gru':
            self.rnn = nn.GRU(input_size = emb_dim,
                              hidden_size = enc_hid_dim,
                              num_layers=num_layers,
                              dropout = dropout,
                              bidirectional= bidirectional,
                              batch_first=True)


    def forward(self, src, hidden, cell=None):
        embedded = self.embedding(src)
        embedded = self.dropout(embedded)

        if self.cell_type == 'lstm':
            output,(hidden,cell) = self.rnn(embedded, (hidden,cell))
        else:
            output, hidden = self.rnn(embedded, hidden)

        return output, hidden, cell

"""## DECODER PART"""

# Decoder part
class Decoder(nn.Module):
    def __init__(self,
                 output_dim = 29,
                 emb_dim = 256,
                 dec_hid_dim = 256,
                 cell_type='gru',
                 num_layers=2,
                 dropout = 0,
                 bidirectional = True,
                 attention = False,
                 attention_dim = None
                 ):

        super(Decoder, self).__init__()
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        # Embedding part
        self.embedding = nn.Embedding(output_dim, emb_dim)
        self.attention = attention
        # Dropout to add onto embedded input
        self.dropout = nn.Dropout(dropout)
        self.cell_type = cell_type
        if bidirectional :
            self.val_direction = 2
        else :
            self.val_direction = 1

        # Linear layer to get the output
        self.W1 = nn.Linear(dec_hid_dim * self.val_direction, output_dim)
        # Softmax layer
        self.softmax = F.softmax

        if attention:
            self.attention_dim = attention_dim
            self.input_size += self.attention_dim

            # Attention layer parameters
            self.U = nn.Sequential(nn.Linear( self.hidden_dimension, self.hidden_dimension), nn.LeakyReLU())
            self.W = nn.Sequential(nn.Linear( self.hidden_dimension, self.hidden_dimension), nn.LeakyReLU())
            self.V = nn.Sequential(nn.Linear( self.hidden_dimension, self.attention_out_dimension), nn.LeakyReLU())


        if cell_type.lower() == 'rnn':
            self.rnn = nn.LSTM(emb_dim,
                               dec_hid_dim,
                               num_layers=num_layers,
                               dropout = dropout,
                               bidirectional= bidirectional,
                               batch_first=True)
        elif cell_type.lower() == 'lstm':
            self.rnn = nn.LSTM(emb_dim,
                               dec_hid_dim,
                               num_layers=num_layers,
                               dropout = dropout,
                               bidirectional= bidirectional,
                               batch_first=True)
        elif cell_type.lower() == 'gru':
            self.rnn = nn.GRU(emb_dim,
                              dec_hid_dim,
                              num_layers=num_layers,
                              dropout = dropout,
                              bidirectional= bidirectional,
                              batch_first=True)

        self.fc_out = nn.Linear(dec_hid_dim, output_dim)

    def calculate_attention(hidden, encoder_outputs, enc_hid_dim=256, dec_hid_dim=256):
        attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)
        v = nn.Linear(dec_hid_dim, 1, bias = False)

        batch_size = encoder_outputs.shape[1]
        src_len = encoder_outputs.shape[0]

        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)
        encoder_outputs = encoder_outputs.permute(1, 0, 2)

        energy = torch.tanh(attn(torch.cat((hidden, encoder_outputs), dim = 2)))
        attention = v(energy).squeeze(2)

        attention_weights = F.softmax(attention, dim=1)
        normalized_context_vector = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)

        return normalized_context_vector, attention_weights

    def forward(self, input, hidden, cell=None,encoder_outputs=None):
#         Incorporate dropout in embedding.
        output = self.dropout(self.embedding(input))

        attention_weights = None
#         If we are using attention, then we need to concatenate the context vector, which we obtain from attention

        if self.attention:
            context,attention_weights = self.calculate_attention(hidden, encoder_outputs)
            output = torch.cat((output,context),2)
        #print(hidden.shape)
        #print(cell)
        if self.cell_type == 'lstm':
            output,(hidden,cell) = self.decoder_type(output,(hidden,cell))
        else:

            output, hidden = self.decoder_type(output, hidden)


        output = self.W1(output)

        return output, hidden, cell, attention_weights

"""## SEQ2SEQ MODEL"""

class Seq2Seq(nn.Module):

    def __init__(self,
                 encoder,
                 decoder,
                 dec_inp_dim = 29,
                 enc_hid_dim = 256,
                 dec_hid_dim =256,
                 bidirectional = True,
                 enc_num_layers = 3,
                 dec_num_layers = 2,
                 cell_type = 'lstm',
                 dropout = 0.2,
                 attention = False
                ):


        super(Seq2Seq, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        # Decoder input dimension
        self.dec_inp_dim = dec_inp_dim

        self.enc_hid_dim = enc_hid_dim

        # Decoder hidden dimension
        self.dec_hid_dim = dec_hid_dim

        # Whether to use bidirectional or not
        self.direction = bidirectional
        self.val_direection = 2 if bidirectional else 1

        # Number of layers for encoder and decoder
        self.enc_num_layers = enc_num_layers
        self.dec_num_layers = dec_num_layers

        # Which cell type to use
        self.cell_type = cell_type

        # Whether to use dropout or not
        self.dropout = dropout

        self.softmax = F.softmax

        # Attention mechanism
        self.attention = attention

        # If attention is used, then we need to transform encoder's last hidden to decoder's first hidden
        self.enc_dec_linear1 = nn.Linear(enc_hid_dim,dec_hid_dim)
        self.enc_dec_linear2 = nn.Linear(enc_num_layers*self.val_direection,dec_num_layers*self.val_direection)

        # Linear layer to transform encoder's last cell to decoder's first cell
        self.enc_dec_cell_linear1 = nn.Linear(enc_hid_dim,dec_hid_dim)
        self.enc_dec_cell_linear2 = nn.Linear(enc_num_layers*self.val_direection,dec_num_layers*self.val_direection)

        # Linear layer to transform encoder's last hidden to decoder's first hidden
        self.enc_dec_att_linear1 = nn.Linear(enc_hid_dim,dec_hid_dim)
        self.enc_dec_att_linear2 = nn.Linear(enc_num_layers*self.val_direection,dec_num_layers*self.val_direection)



    def forward(self, source, target, teacher_forcing = False):
        batch_size = source.shape[0]
        hidden_dim = torch.zeros(self.val_direction*self.num_layers, batch_size, self.enc_hid_dim, device=device)
        cell_state = torch.zeros(self.val_direction*self.num_layers, batch_size, self.enc_hid_dim, device=device) if self.cell_type == 'lstm' else None
        encoder_outputs = torch.zeros(32,self.val_direection*self.dec_num_layers,batch_size,self.dec_hid_dim,device=device) if self.attention else None

        # Pass source to encoder one character at a time in batch fashion
        for t in range(32):
            encoder_output, hidden_dim, cell_state = self.encoder.process(source[:,t].unsqueeze(0), hidden_dim, cell_state)
            if self.attention:
                hidden_dim_new = self.enc_dec_att_linear1(hidden_dim).permute(2,1,0).contiguous()
                hidden_dim_new = self.enc_dec_att_linear2(hidden_dim_new).permute(2,1,0).contiguous()
                encoder_outputs[t] = hidden_dim_new

        # Encoder's last state is decoders first state
        last_state = hidden_dim
        predictions = torch.zeros(29, batch_size, self.dec_inp_dim,device = device)
        attention_weights = torch.zeros(29, 32, self.val_direection*self.dec_num_layers ,batch_size, device = device)
        decoder_hidden = self.enc_dec_linear1(last_state).permute(2,1,0).contiguous()
        decoder_hidden = self.enc_dec_linear2(decoder_hidden).permute(2,1,0).contiguous()

        # Here also, encoders last cell is decoders first cell, also transform to same dimension
        if  self.cell_type == 'lstm':
            cell_state = self.enc_dec_cell_linear1(cell_state).permute(2,1,0).contiguous()
            cell_state = self.enc_dec_cell_linear2(cell_state).permute(2,1,0).contiguous()

        # output at start is all 1's <SOS>
        output = torch.ones(1,batch_size,dtype=torch.long, device=device)
        predictions[0,:,1]=torch.ones(batch_size)
        attention_wts = None

        # Do decoding by char by char fashion by batch
        for t in range(1,29):
            if teacher_forcing:
                output, decoder_hidden, cell_state, attention_wts = self.decoder.process(target[:,t-1].unsqueeze(0), decoder_hidden, cell_state, encoder_outputs)
                predictions[t] = output.squeeze(0)
            else:
                output, decoder_hidden, cell_state, attention_wts = self.decoder.process(output, decoder_hidden, cell_state, encoder_outputs)
                predictions[t] = output.squeeze(0)
                if self.attention:
                    attention_weights[t] = attention_wts.squeeze(3)
                output = torch.argmax(self.softmax(output,dim=2),dim=2)

        return predictions, attention_wts

"""## TRAINING AND ACCURACY"""

def train1(model, train_loader, val_loader, epochs):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    optimizer = optim.Adam(model.parameters())
    criterion = nn.CrossEntropyLoss()
    model.train()

    for epoch in tqdm(range(epochs)):
        for phase in ['train', 'val']:
            if phase == 'train':
                data_loader = train_loader
            else:
                data_loader = val_loader

            running_loss = 0.0
            running_corrects = 0

            for inputs, labels in data_loader:
                inputs = inputs.to(device)
                labels = labels.to(device)

                optimizer.zero_grad()

                with torch.set_grad_enabled(phase == 'train'):
                    outputs, _ = model(inputs, labels, epoch < epochs/2, phase == 'train')
                    outputs = outputs.permute(1, 0, 2).reshape(-1, 72)
                    labels = F.one_hot(labels, num_classes=72).float().reshape(-1, 72)
                    loss = criterion(outputs, labels)

                    if phase == 'train':
                        loss.backward()
                        nn.utils.clip_grad_norm_(model.parameters(), 1)
                        optimizer.step()

                running_loss += loss.item() * inputs.size(0)
                preds = torch.argmax(F.softmax(outputs, dim=1), dim=1)
                running_corrects += torch.sum(preds == labels.data)

            epoch_loss = running_loss / len(data_loader.dataset)
            epoch_acc = running_corrects.double() / len(data_loader.dataset)
            print(f'Epoch no: {epoch}')
            if phase == 'train':
                print(f'Train loss: {epoch_loss} \t Train Accuracy: {epoch_acc}')
            else:
                print(f'Validation loss: {valid_loss} \t Validation Accuracy: {val_acc}')
                wandb.log({ 'Epoch': epoch, 'validation_accuracy': epoch_acc * 100})
                wandb.log({ 'Epoch': epoch, 'validation_loss': epoch_loss * 100})

"""## WANDB SWEEPS WITHOUT ATTENTION"""

# sweep config file
sweep_config = {
    'method': 'bayes',
    'name' : 'sweep - no attention',
    'metric': {
      'goal': 'maximize',
      'name': 'validation_accuracy'
    },
    'parameters':{
        'input_embedding_size': {
            'values': [64, 128] # 16,32,64,
        },
        'enc_layers': {
            'values': [1,2,3]
        },
        'dec_layers': {
            'values': [1,2,3]
        },
        'hidden_size': {
            'values': [64, 128, 256]
        },
        'cell_type': {
            'values': ['lstm','rnn','gru']
        },
        'bidirectional' : {
            'values' : [True]
        },
        'dropout': {
            'values': [0.1, 0.2, 0.3]
        },
        'beam_size' : {
            'values' : [1,3,5]
        }
     }
}

# Create a sweep
sweep_id = wandb.sweep(sweep = sweep_config, entity="abhinavtk", project='MA23M002-A3') # modeling MA23M002-A3
# 4966bf774ea7ce7b47a6e7360b1fad927ece6f1c

#wandb sweeps without attention - make sure to use your key for running
def main():
  with wandb.init() as run:
    wandb.run.name = f'cell-{wandb.config.cell_type}_hid_sz-{wandb.config.hidden_size}_inp_embed-{wandb.config.input_embedding_size}_enc-{wandb.config.enc_layers}_dec-{wandb.config.dec_layers}_dropout-{wandb.config.dropout}'

    # Encoder part
    encoder = Encoder(
                    embed_dimension = wandb.config.input_embedding_size,
                    hidden_dimension =  wandb.config.hidden_size,
                    cell_type = wandb.config.cell_type,
                    layers = wandb.config.enc_layers,
                    bidirectional = wandb.config.bidirectional,
                    dropout = wandb.config.dropout
                    )
    # decoder
    decoder = Decoder(
                        emb_dim = wandb.config.input_embedding_size,
                        dec_hid_dim = wandb.config.hidden_size,
                        cell_type = wandb.config.cell_type,
                        num_layers = wandb.config.dec_layers,
                        dropout = wandb.config.dropout,
                        bidirectional = wandb.config.bidirectional,
                        attention = False,
                        attention_dim = wandb.config.hidden_size
                        )
    # Init model
    model1 = Seq2Seq(encoder,
                     decoder,
                    enc_hid_dim = wandb.config.hidden_size,
                    dec_hid_dim = wandb.config.hidden_size,
                    bidirectional = wandb.config.bidirectional,
                    encoder_num_layers = wandb.config.enc_layers,
                    decoder_num_layers = wandb.config.dec_layers,
                    cell_type = wandb.config.cell_type,
                    dropout = wandb.config.dropout,
                    attention = False
                )

    model1.to(device)

    epochs = 15
    train1(model1, train_loader, val_loader, epochs)

wandb.agent(sweep_id, function = main, count = 50) # calls main function for count number of times
wandb.finish()



"""# Test Accuracy calculation"""

def calc_test_acc(model, loader):
    model.eval()
    total_loss = 0
    total_score = 0
    criterion = nn.CrossEntropyLoss()

    for inputs, targets in loader:
        inputs, targets = inputs.to(device), targets.to(device)
        outputs, _ = model(inputs, None, False, False)

        # Calculate accuracy
        preds = torch.argmax(F.softmax(outputs, dim=2), dim=2).T
        total_score += compute_score(preds, targets)

        # Reshape outputs and targets for loss calculation
        outputs = outputs.permute(1, 0, 2).reshape(-1, 72)
        targets = F.one_hot(targets, num_classes=72).float().reshape(-1, 72)

        # Calculate loss
        loss = criterion(outputs, targets)
        total_loss += loss.item()

    avg_loss = total_loss / len(loader)
    avg_score = total_score / len(loader.dataset)

    print(f'Test Loss: {avg_loss} \t Test Accuracy: {avg_score}')

    wandb.log({'Test_accuracy': avg_score * 100})
    wandb.log({ 'Test_loss': avg_loss})

# Best hyperparameter configuration without attention
'''
input embedding size: 256
number of encoder layers: 3
number of decoder layers: 2
hidden layer size: 256
cell type: LSTM
bidirectional: True
dropout: 0.2
beam width : 1
'''
best_model = Seq2Seq(
    encoder_hidden_dimension = 256,
    decoder_hidden_dimension =256,
    encoder_embed_dimension = 256,
    decoder_embed_dimension = 256,
    bidirectional = True,
    encoder_num_layers = 3,
    decoder_num_layers = 2,
    cell_type = 'lstm',
    dropout = 0.3,
    beam_width = 5,
    device = device,
    attention = False
)
best_model.to(device)
epochs = 20
train1(best_model, train_loader, val_loader, epochs, False)

# sweep config file
sweep_config = {
    'method': 'grid',
    'name' : 'testset run',
    'metric': {
      'goal': 'maximize',
      'name': 'test_accuracy'
    },
    'parameters': {
        'beam_size':{
            'values': [1]
        }
    }
}

# Create a sweep
sweep_id = wandb.sweep(sweep = sweep_config, entity="abhinavtk", project='MA23M002-A3')

# wandb log for test accuracy
def main():
  with wandb.init() as run:
    #run_name = "-f_num_"+str(wandb.config.filters_num)+"-f_num_"+wandb.config.filter_org+"-ac_fn_"+wandb.config.act_fn+\
                #"-b_norm_"+str(wandb.config.batch_norm) + "-bs_"+str(wandb.config.batch_size) +"-neu_num"+str(wandb.config.num_neurons_dense)

    wandb.run.name = "test_set_run"
    calc_test_acc(best_model, test_loader)

wandb.agent(sweep_id, function = main, count = 1)
wandb.finish()

"""# wandb sweeps with attention"""

# sweep config file
sweep_config = {
    'method': 'bayes',
    'name' : 'sweep - attention',
    'metric': {
      'goal': 'maximize',
      'name': 'validation_accuracy'
    },
    'parameters':{
        'input_embedding_size': {
            'values': [32, 64, 128] # 16,32,64,
        },
        'enc_layers': {
            'values': [1,2,3]
        },
        'dec_layers': {
            'values': [1,2,3]

        },
        'hidden_size': {
            'values': [64, 128, 256]
        },
        'cell_type': {
            'values': ['lstm','rnn','gru']
        },
        'bidirectional' : {
            'values' : [True]
        },
        'dropout': {
            'values': [0.1, 0.2, 0.3]
        },
        'beam_size' : {
            'values' : [1,3,5]
        }
     }
}

# Create a sweep
sweep_id = wandb.sweep(sweep = sweep_config, entity="abhinavtk", project='MA23M002-A3')

#wandb sweeps with attention
def main():
  with wandb.init() as run:
    wandb.run.name = f'cell-{wandb.config.cell_type}_hid_sz-{wandb.config.hidden_size}_inp_embed-{wandb.config.input_embedding_size}_enc-{wandb.config.enc_layers}_dec-{wandb.config.dec_layers}_dropout-{wandb.config.dropout}'

    model1 = Seq2Seq(
        encoder_hidden_dimension = wandb.config.hidden_size,
        decoder_hidden_dimension = wandb.config.hidden_size,
        encoder_embed_dimension =  wandb.config.input_embedding_size,
        decoder_embed_dimension =  wandb.config.input_embedding_size,
        bidirectional = wandb.config.bidirectional,
        encoder_num_layers = wandb.config.enc_layers,
        decoder_num_layers = wandb.config.dec_layers,
        cell_type = wandb.config.cell_type,
        dropout = wandb.config.dropout,
        beam_width = wandb.config.beam_size,
        device = device,
        attention = True
    )


    model1.to(device)
    beam = False

    epochs = 15
    train1(model1, train_loader, val_loader, epochs, beam)

wandb.agent(sweep_id, function = main, count = 25) # calls main function for count number of times
wandb.finish()

# Best hyperparameter configuration with attention
'''
input embedding size: 256
number of encoder layers: 3
number of decoder layers: 3
hidden layer size: 256
cell type: LSTM
bidirectional: Yes
dropout: 0.3
beam width : 1
'''
best_model_attn = Seq2Seq(
    encoder_hidden_dimension = 256,
    decoder_hidden_dimension =256,
    encoder_embed_dimension = 256,
    decoder_embed_dimension = 256,
    bidirectional = True,
    encoder_num_layers = 3,
    decoder_num_layers = 3,
    cell_type = 'lstm',
    dropout = 0.3,
    beam_width = 1,
    device = device,
    attention = True
)
best_model_attn.to(device)
epochs = 20
train1(best_model_attn, train_loader, val_loader, epochs, False)

# sweep config file
sweep_config = {
    'method': 'grid',
    'name' : 'testset run attention',
    'metric': {
      'goal': 'maximize',
      'name': 'test_accuracy'
    },
    'parameters': {
        'beam_size':{
            'values': [1]
        }
    }
}
sweep_id = wandb.sweep(sweep = sweep_config, entity="abhinavtk", project='modeling')

# wandb log for test accuracy
def main():
  with wandb.init() as run:
    #run_name = "-f_num_"+str(wandb.config.filters_num)+"-f_num_"+wandb.config.filter_org+"-ac_fn_"+wandb.config.act_fn+\
                #"-b_norm_"+str(wandb.config.batch_norm) + "-bs_"+str(wandb.config.batch_size) +"-neu_num"+str(wandb.config.num_neurons_dense)

    wandb.run.name = "test_set_run_attn"
    calc_test_acc(best_model_attn, test_loader)

wandb.agent(sweep_id, function = main, count = 1)
wandb.finish()

"""# Attention heatmap plot and logging it in wandb"""

test_input, test_labels = next(iter(test_loader))
best_model.eval()
test_output,_weights = best_model.forward(test_input.to(device), None,False)

# Plot attention hmap
def prepare_ticks(input_data, output_data, index):
    filter_func = lambda x: x.item() not in [0, 1, 2]
    x_ticks = [english_index_dict[i.item()] for i in input_data[index] if filter_func(i)]
    y_ticks = [malayalam_index_dict[i.item()] for i in output_data[index] if filter_func(i)]
    return x_ticks, y_ticks

def generate_heatmap(input_data, output_data, weights, num_plots=12):
    fig, ax = plt.subplots(4, 3, figsize=(20, 20))
    plt.setp(ax)

    for idx in range(num_plots):
        x_ticks, y_ticks = prepare_ticks(input_data, output_data, idx)
        heatmap_data = weights[:, :, idx].detach().cpu().numpy()
        heatmap_data = heatmap_data[1:len(y_ticks)+1, 2:len(x_ticks)+2]

        plt.sca(ax[idx//3, idx%3])
        plt.imshow(heatmap_data, interpolation='nearest', cmap='inferno')
        plt.colorbar()
        plt.xticks(np.arange(0, len(x_ticks)), x_ticks)

        mal_font = FontProperties(fname='/kaggle/input/attention-heatmap/AnjaliOldLipi-Regular.ttf')
        plt.yticks(np.arange(0, len(y_ticks)), y_ticks, fontproperties=mal_font)

        plt.xlabel('English')
        plt.ylabel('Malayalam')
        plt.title(f'Sample {idx + 1}')

    plt.show()

    canvas = plt.gca().figure.canvas
    canvas.draw()
    image_data = np.frombuffer(canvas.tostring_rgb(), dtype=np.uint8)
    image = image_data.reshape(canvas.get_width_height()[::-1] + (3,))

    return image

output_softmax = F.softmax(test_output, dim=2)
output_argmax = torch.argmax(output_softmax, dim=2).T
mean_weights = torch.mean(_weights, axis=2)
image = generate_heatmap(test_input, output_argmax, mean_weights)

sweep_config = {
    'method': 'grid',
    'name' : 'attention_plot',
    'parameters': {
        'beam_size': {
            'values': [1]
        }
  }
}

sweep_id = wandb.sweep(sweep = sweep_config, entity="abhinavtk", project='MA23M002-A3')

def main():
    with wandb.init() as run:
        #run_name = "-f_num_"+str(wandb.config.filters_num)+"-f_num_"+wandb.config.filter_org+"-ac_fn_"+wandb.config.act_fn+\
                    #"-b_norm_"+str(wandb.config.batch_norm) + "-bs_"+str(wandb.config.batch_size) +"-neu_num"+str(wandb.config.num_neurons_dense)

        wandb.run.name = "attention_heatmap"
        #wandb.log({"image_pred": [wandb.Image(image, caption="Test Images and Predictions")]})
        wandb.log({"image_grid": [wandb.Image(image, caption="Attention Heatmap")]})
wandb.agent(sweep_id, function = main, count = 1)
wandb.finish()